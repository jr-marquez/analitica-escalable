Dentro del directorio SparkDocker:
docker build . -t cluster-apache-spark:3.3.0 --> crea imagen con nombre cluster-apache-spark utilizando el Dockerfile ubicado en . (es decir, estoy ejecutando el comando en la misma ruta del Dockerfile)
En el directorio raiz: 
docker-compose -f docker-compose-spark.yaml up -d --> ejecuta varios containers y crea una unica red interna. Esto se define dentro del archivo de Docker Compose (herramienta que facilita la vida)
docker-compose -f docker-compose-spark.yaml down -v --> detenemos los contenedores y borramos cualquier volumen temporal

docker tag 11a2db475710 ramongo/cluster-apache-spark:3.3.0
docker push ramongo/cluster-apache-spark:3.3.0
